{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lVuNkJP8TZ6"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "In this project, we build a machine learning model capable of evaluating the humor quality of short stand-up comedy clips. The goal is to predict how well a given audio performance will be received by a real audience, based on its content and delivery.\n",
        "\n",
        "To achieve this, we work with a dataset of stand-up audio clips and their transcriptions. Our system will analyze features such as:\n",
        "\n",
        "Overall ranking of how funny it will be based on total laughter time to total speech time\n",
        "Audience engagement patterns (word level response)\n",
        "Linguistic style and structure of jokes\n",
        "Emotional tone and delivery cues (e.g., timing, pacing, pauses)\n",
        "Laughter reactions (presence, duration) (intensity?)\n",
        "\n",
        "\n",
        "The project involves:\n",
        "\n",
        "Text preprocessing: transcript text.\n",
        "\n",
        "Audio feature extraction: Detecting laughter segments and other prosodic cues.\n",
        "\n",
        "Embedding and modeling: Using pre-trained language models (e.g., BERT) or training a humor-specific embedding.\n",
        "\n",
        "Prediction & critique: Predicting an overall humor rating (e.g., 1â€“5) and providing fine-grained feedback to the performer.\n",
        "\n",
        "Ultimately, this model can support comedians in refining their material, help platforms surface funnier content, and enable deeper understanding of what makes something â€œfunny.â€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZRlpUBL8hxg"
      },
      "source": [
        "## Data dictionary\n",
        "\n",
        "speech: the text transcription of the standup audio clip\n",
        "\n",
        "ranking: the ranking in the sclae of 1-4\n",
        "\n",
        "audio feature 1:\n",
        "\n",
        "audio feature 2:\n",
        "\n",
        "audio feature 3:\n",
        "\n",
        "audio feature 4:\n",
        "\n",
        "audio feature 5:\n",
        "\n",
        "audio feature 6:\n",
        "\n",
        "duration (optional): the total time length of the speech\n",
        "\n",
        "laugh time (optional): the total length of all laughters\n",
        "\n",
        "\n",
        "Word level\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7c6JjN94Fk"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TU9GUgBQ9vZB",
        "outputId": "485df563-da4a-42f5-a362-fdaf94b19e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing the libraries with specified versions\n",
        "!pip install -U -q sentence-transformers==4.1.0 transformers==4.52.4 bitsandbytes==0.46.0 accelerate==1.7.0 sentencepiece==0.2.0 pandas==2.2.2 numpy==2.0.2 matplotlib==3.10.0 seaborn==0.13.2 torch==2.6.0 scikit-learn==1.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ivbPaoT9xaH"
      },
      "outputs": [],
      "source": [
        "# to read and manipulate the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_colwidth', None)  # setting column to the maximum column width as per the data\n",
        "\n",
        "# to visualise data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Deep Learning library\n",
        "import torch\n",
        "\n",
        "# to load transformer models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
        "\n",
        "# to split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to compute performance metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# To build a Random Forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# to ignore unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLTc3HcYWmJj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Encode audio and text into a shared vector space,\n",
        "allowing for multimodal tasks like cross-modal search or improved classification\n",
        "\n",
        "* audio feature granulaty: word level or sentence level??)\n",
        "  * intuitively, it makes sense to integret word level audio feature with each text token's during embedding process? how to realize techinically??\n",
        "\n",
        "* How to integrete with text embedding: add, concatenate, or other??\n",
        "\n",
        "* Currently has word level laughter info\n",
        "* other audio feature sentence level?: removing laughter, extracting relevant features that capture aspects like pitch, rhythm, timbre, and even speech patterns, like Mel Frequency Cepstral Coefficients (MFCCs) are often used, along with deep learning models like WaveNet or DeepSpeech for feature extraction."
      ],
      "metadata": {
        "id": "IMsbPFlbX4lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Audio features"
      ],
      "metadata": {
        "id": "RsYF0MJMmDqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "\n",
        "from scipy.signal import lfilter\n",
        "from scipy.fftpack import dct\n",
        "from scipy.signal import spectrogram\n",
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "IqOuMcd4Yfoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MP3 file\n",
        "audio_path = \"/mnt/data/1.mp3\"\n",
        "audio = AudioSegment.from_file(audio_path)"
      ],
      "metadata": {
        "id": "xDsXIzEZYtpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot waveform graph"
      ],
      "metadata": {
        "id": "tQFBLxBFYRnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to mono and get raw data\n",
        "samples = np.array(audio.set_channels(1).get_array_of_samples())\n",
        "\n",
        "# Normalize samples to range [-1, 1]\n",
        "samples = samples.astype(np.float32) / (2**15)\n",
        "\n",
        "# Create a time axis\n",
        "duration = len(audio) / 1000.0  # in seconds\n",
        "time = np.linspace(0, duration, num=len(samples))\n",
        "\n",
        "# Plot waveform\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(time, samples, linewidth=0.5)\n",
        "plt.title(\"Audio Waveform of Stand-Up Clip\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k7qcxHxSmK2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio features:\n",
        "* MFCCs (Mel Frequency Cepstral Coefficients): Capture the timbral texture of the voice, often used in speech and emotion recognition.\n",
        "* RMS Energy: Measures the loudness of the signal. Higher energy may correlate with intense delivery or audience laughter.\n",
        "* Spectrogram (dB scale): Visualizes frequency over time. Dense regions may hint at dynamic tone or punchlines.\n",
        "  * spectral_centroid: Brightness or average frequency\tHighlights expressive voice tone\n",
        "  * spectral_rolloff: Frequency where energy drops off\tDetects sharp or fast-paced delivery\n",
        "  * spectral_contrast: Dynamic range across frequency bands\tCaptures voice expressiveness & emotion\n",
        "* Line Spectral Frequencies (LSF): Represent the speech spectral envelope and help capture vocal tract information, useful for distinguishing speaker tone or emotion.\n",
        "* Zero-Crossing Rate (ZCR): Measures signal noisiness or fricative speech; higher in laughter or fast-paced delivery.\n",
        "* Delta Coefficients of MFCCs: Capture temporal changes in MFCCsâ€”important for modeling how humor builds up or shifts across a performance."
      ],
      "metadata": {
        "id": "-M5hWr2oYHWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load audio file\n",
        "y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "# --- Feature 1: MFCCs ---\n",
        "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "# --- Feature 2: RMS Energy ---\n",
        "rms = librosa.feature.rms(y=y)\n",
        "rms_mean = rms.mean()\n",
        "\n",
        "# --- Feature 3 : Spectrogram ---\n",
        "S = np.abs(librosa.stft(y))\n",
        "spectrogram_db = librosa.amplitude_to_db(S, ref=np.max)\n",
        "spectrogram_mean = spectrogram_db.mean()\n",
        "\n",
        "spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "\n",
        "# --- Feature 4: Line Spectral Frequencies (LSF) ---\n",
        "def lpc(signal, order):\n",
        "    \"\"\"Compute Linear Predictive Coefficients using autocorrelation.\"\"\"\n",
        "    from scipy.linalg import toeplitz, solve_toeplitz\n",
        "    R = np.correlate(signal, signal, mode='full')\n",
        "    R = R[len(R)//2:]\n",
        "    r = R[1:order+1]\n",
        "    R_matrix = toeplitz(R[:order])\n",
        "    lpc_coeffs = solve_toeplitz((R[:order], R[:order]), r)\n",
        "    return np.concatenate(([1], -lpc_coeffs))\n",
        "\n",
        "def lsf_from_lpc(a):\n",
        "    \"\"\"Convert LPC to LSF using polynomial root finding.\"\"\"\n",
        "    import numpy.polynomial.polynomial as poly\n",
        "    A = a\n",
        "    P = A + np.flip(A)\n",
        "    Q = A - np.flip(A)\n",
        "    roots_P = np.roots(P)\n",
        "    roots_Q = np.roots(Q)\n",
        "    angles_P = np.angle(roots_P[np.isreal(roots_P)])\n",
        "    angles_Q = np.angle(roots_Q[np.isreal(roots_Q)])\n",
        "    return np.sort(np.concatenate((angles_P, angles_Q)))\n",
        "\n",
        "lpc_coeffs = lpc(y[:2048], order=10)\n",
        "lsf = lsf_from_lpc(lpc_coeffs)\n",
        "lsf_mean = np.mean(lsf)\n",
        "\n",
        "# --- Feature 5: Zero-Crossing Rate ---\n",
        "zcr = librosa.feature.zero_crossing_rate(y)\n",
        "zcr_mean = zcr.mean()\n",
        "\n",
        "# --- Feature 6: Delta Coefficients of MFCCs ---\n",
        "delta_mfcc = librosa.feature.delta(mfccs)\n",
        "delta_mfcc_mean = delta_mfcc.mean(axis=1)\n",
        "\n",
        "# --- Combine All Features into DataFrame ---\n",
        "# recommend using mean values to reduce dimensionalitym for base model, add full array for most importanct features during fine tune.\n",
        "# feature_dict = {\n",
        "#     **{f\"mfcc_{i+1}\": mfccs_mean[i] for i in range(len(mfccs_mean))},\n",
        "#     \"rms_energy\": rms_mean,\n",
        "#     \"spectrogram_db_mean\": spectrogram_mean,\n",
        "#     \"spectral_centroid\": spectral_centroid.tolist(),\n",
        "#     \"spectral_rolloff\": spectral_rolloff.tolist(),\n",
        "#     \"spectral_contrast\": spectral_contrast.tolist(),\n",
        "#     \"lsf_mean\": lsf_mean,\n",
        "#     \"zcr_mean\": zcr_mean,\n",
        "#     **{f\"delta_mfcc_{i+1}\": delta_mfcc_mean[i] for i in range(len(delta_mfcc_mean))}\n",
        "# }\n",
        "feature_dict = {\n",
        "    **{f\"mfcc_{i+1}\": mfccs_mean[i] for i in range(len(mfccs_mean))},\n",
        "    \"rms_energy\": rms_mean,\n",
        "    \"spectrogram_db_mean\": spectrogram_mean,\n",
        "    \"spectral_centroid\": spectral_centroid_mean,\n",
        "    \"spectral_rolloff\": spectral_rolloff_mean,\n",
        "    \"spectral_contrast\": spectral_contrast_mean,\n",
        "    \"lsf_mean\": lsf_mean,\n",
        "    \"zcr_mean\": zcr_mean,\n",
        "    **{f\"delta_mfcc_{i+1}\": delta_mfcc_mean[i] for i in range(len(delta_mfcc_mean))}\n",
        "}\n",
        "\n",
        "audio_features = pd.DataFrame([feature_dict])\n",
        "audio_features.head()\n"
      ],
      "metadata": {
        "id": "Xvnb1RRqXrCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdvqkxgP3Hip"
      },
      "source": [
        "# Import text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2KJT6V_UV1s"
      },
      "source": [
        "2070 files uploaded to google dirve, one file upload failed S_ITYFTLT_audio_6.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPvGy_1Yz8LZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3hETlbHW2x0L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AhCIJXmH0AhQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the input folder and output file\n",
        "input_folder = \"/content/drive/MyDrive/AI_open_mic_dataset\"\n",
        "output_csv = \"funny.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_LMDNJjzq2w"
      },
      "outputs": [],
      "source": [
        "# Prepare a list to store data\n",
        "data_rows = []\n",
        "\n",
        "# Walk through all files in the folder\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                content = file.read().strip()\n",
        "                data_rows.append({\"text\": content, \"file name\": file_name})\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_name}: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqwMpB2U0O6W"
      },
      "outputs": [],
      "source": [
        "data_rows.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Q0L8X80OaI"
      },
      "outputs": [],
      "source": [
        "# Write to CSV\n",
        "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    fieldnames = [\"text\", \"file name\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data_rows)\n",
        "\n",
        "print(f\"Extracted {len(data_rows)} .txt files into {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFGzL6DD3ChC"
      },
      "source": [
        "# Data overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taTBTIJz5rlu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdPSA2wq6ePA"
      },
      "source": [
        "# Text embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BVgMO3v3No6"
      },
      "source": [
        "## Use an existing BERT model to do text embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP6Q397t4TaZ"
      },
      "source": [
        "We'll be using the all-MiniLM-L6-v2 model here.\n",
        "\n",
        "ğŸ’¡ The all-MiniLM-L6-v2 model is an all-round (all) model trained on a large and diverse dataset of over 1 billion training samples and generates state-of-the-art sentence embeddings of 384 dimensions.\n",
        "\n",
        "ğŸ“Š It is a language model (LM) that has 6 transformer encoder layers (L6) and is a smaller model (Mini) trained to mimic the performance of a larger model (BERT).\n",
        "\n",
        "ğŸ› ï¸ Potential use-cases include text classification, sentiment analysis, and semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOGModiO4OtG"
      },
      "outputs": [],
      "source": [
        "# defining the model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiXxU31o4mGo"
      },
      "outputs": [],
      "source": [
        "model.encode(['I like clean jokes!'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhEtHZDm78QG"
      },
      "outputs": [],
      "source": [
        "# setting the device to GPU if available, else CPU\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecYVBqVW8H3s"
      },
      "outputs": [],
      "source": [
        "# encoding the dataset\n",
        "embedding_matrix = model.encode(data['review'], device=device, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcxV-Auf5-SK"
      },
      "source": [
        "## Use Word2Vec for text embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12GwkMtSbIlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YINIlCK86TqY"
      },
      "source": [
        "## Fine tune a LM model with funny text for text embedding?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "996LnbL7b5MC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nW6R5axQbJlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA5-LgpLCopj"
      },
      "source": [
        "## Build own transformer encoder using mostly funny text\n",
        "$$$ billions"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wa62w84-bKYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Encode audio and text into a shared vector space,\n",
        "allowing for multimodal tasks like cross-modal search or improved classification\n",
        "\n",
        "* (add, concatenate, or other???)\n",
        "\n",
        "\n",
        "Audio: removing laughter, extracting relevant features that capture aspects like pitch, rhythm, timbre, and even speech patterns. Techniques like Mel Frequency Cepstral Coefficients (MFCCs) are often used, along with deep learning models like WaveNet or DeepSpeech for feature extraction.\n",
        "\n",
        "Text: Preprocessing includes cleaning the text, tokenizing it into individual words or sub-word units, and converting it into embeddings that capture semantic meaning. Models like BERT, GPT, and T5 are widely used for generating text embeddings.\n",
        "* sentence level embedding"
      ],
      "metadata": {
        "id": "aGWWhEw9nQB5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DW679UzXbM4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVukc8uV6_uY"
      },
      "source": [
        "# Humor analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJevPipY-m5l"
      },
      "source": [
        "## Use nueral network, input sentence level embedding, hidden layers, then output a continuous ranking - this will hardly work since it does not have sequence\n",
        "## transformer vs. LSMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2iZcBoi6Oyl"
      },
      "source": [
        "## Use LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJfELv1obZXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au-fv4es-Y1g"
      },
      "source": [
        "## Use Fine tuned LLM with funny text"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WLtaiogbaQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btirk4Ux6Dka"
      },
      "source": [
        "## Random forest, XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLl_hVXhDeI1"
      },
      "outputs": [],
      "source": [
        "# Process the data\n",
        "\n",
        "X = embedding_matrix\n",
        "y = data[\"ranking\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVja2N3ZDoZs"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "rf_transformer = RandomForestClassifier(n_estimators = 100, max_depth = 7, random_state = 42)\n",
        "\n",
        "# Fitting on train data\n",
        "rf_transformer.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaAr3DP9Dyok"
      },
      "outputs": [],
      "source": [
        "# creating a function to plot the confusion matrix\n",
        "def plot_confusion_matrix(actual, predicted):\n",
        "    cm = confusion_matrix(actual, predicted)\n",
        "\n",
        "    plt.figure(figsize = (5, 4))\n",
        "    label_list = [0, 1]\n",
        "    sns.heatmap(cm, annot = True,  fmt = '.0f', xticklabels = label_list, yticklabels = label_list)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrl6AA4pD2-d"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_train, y_pred_train)\n",
        "plot_confusion_matrix(y_test, y_pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh7_WZF_Chap"
      },
      "source": [
        "## Build own Transformer prediction model with Multi-head Attention\n",
        " $$$$$ billions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}